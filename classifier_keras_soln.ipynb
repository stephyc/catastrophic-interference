{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import scipy \n",
    "import tensorflow as tf\n",
    "\n",
    "import imageio\n",
    "import gzip\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm import trange, tqdm\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Optimizer\n",
    "from keras.callbacks import Callback\n",
    "from collections import OrderedDict\n",
    "\n",
    "from helpers import utils\n",
    "# from helpers import protocols\n",
    "# from helpers.keras_utils import LossHistory\n",
    "# from helpers.optimizers import KOOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# extracrt labels from local file\n",
    "train_labels = extract_labels(\"MNIST-data/train-labels-idx1-ubyte.gz\", 60000)\n",
    "eval_labels = extract_labels(\"MNIST-data/t10k-labels-idx1-ubyte.gz\", 10000)\n",
    "y_train = keras.utils.to_categorical(train_labels, num_classes)\n",
    "y_test = keras.utils.to_categorical(eval_labels, num_classes)\n",
    "\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(eval_labels))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONSTRUCT DATASETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local dataset based off of permuted MNIST data\n",
    "def createDataset(name, trainsrc, testsrc):\n",
    "\tprint(\"Beginning import:\", name)\n",
    "\ttrain = np.zeros((train_size, img_rows, img_cols), dtype=np.float32)\n",
    "\ttest = np.zeros((test_size, img_rows, img_cols), dtype=np.float32)\n",
    "\n",
    "\timgstrain = [\"MNIST-processed-training/{0}{1}.png\".format(trainsrc, k) for k in range(1, train_size)]\n",
    "\timgstest = [\"MNIST-processed-test/{0}{1}.png\".format(testsrc, k) for k in range(1, test_size + 1)]\n",
    "\n",
    "\tfor i in range(len(imgstrain)):\n",
    "\t\timg = np.array(Image.open(imgstrain[i]))\n",
    "\t\ttrain[i, :, :] = img\n",
    "\n",
    "\tfor  i in range(len(imgstest)):\n",
    "\t\timg = np.array(Image.open(imgstest[i]))\n",
    "\t\ttest[i, :, :] = img\n",
    "\n",
    "\tprint(\"Completed import:\", name)\n",
    "\n",
    "\treturn (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data params\n",
    "# input_dim = 784\n",
    "# output_dim = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# # Network params\n",
    "# n_hidden_units = 2000\n",
    "# activation_fn = tf.nn.relu\n",
    "\n",
    "# Optimization params\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 5 # epochs per task\n",
    "# learning_rate=1e-3\n",
    "# xi = 0.1\n",
    "\n",
    "# Reset optimizer after each age\n",
    "# reset_optimizer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters\n",
    "img_rows, img_cols = 28, 28\n",
    "train_size = 60000\n",
    "test_size = 10000\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "lr = 0.01\n",
    "xi = 0.1\n",
    "\n",
    "# Architecture params\n",
    "hidden_neurons = 3000\n",
    "activation_fn = tf.nn.relu \n",
    "output_fn = tf.nn.softmax\n",
    "\n",
    "# data\n",
    "input_size = 784\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all permuted datasets (paths = local paths in filesystem)\n",
    "def importData():\n",
    "    srcs = [(\"original\", \"original/original\", \"original/test-original\"),\\\n",
    "\t\t(\"rot90\", \"rot90/rot90\", \"rot90/test-rot90\"), \\\n",
    "\t\t(\"fliplr\", \"fliplr/fliplr\", \"fliplr/test-fliplr\"), \\\n",
    "\t\t(\"flipud\", \"flipud/flipud\", \"flipud/test-flipud\"), \\\n",
    "\t\t(\"check\", \"checkerboard/fullcheck\", \"checkerboard/test-checkerboard\"), \\\n",
    "\t\t(\"inv\", \"Inv/inv\", \"inv/test-inv\"), \\\n",
    "\t\t(\"cutud\",\"cutud/cutUD\", \"cutud/test-cutud\"),\\\n",
    "\t\t(\"invbot\", \"invbot/invbot\", \"invbot/test-invbot\"), \\\n",
    "\t\t]\n",
    "    \n",
    "    datasets = list(map(lambda x: createDataset(x[0], x[1], x[2]), srcs))\n",
    "\n",
    "    data = dict()\n",
    "    \n",
    "    for i in range(len(datasets)):\n",
    "        x_train = datasets[i][0]\n",
    "        x_test = datasets[i][1]\n",
    "\t\t#if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
    "        input_shape = (1, img_rows * img_cols)\n",
    "\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "\n",
    "        data[i] = {\"train\": x_train, \"test\": x_test}\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning import: original\n",
      "Completed import: original\n",
      "Beginning import: rot90\n",
      "Completed import: rot90\n",
      "Beginning import: fliplr\n",
      "Completed import: fliplr\n",
      "Beginning import: flipud\n",
      "Completed import: flipud\n",
      "Beginning import: check\n",
      "Completed import: check\n",
      "Beginning import: inv\n",
      "Completed import: inv\n",
      "Beginning import: cutud\n",
      "Completed import: cutud\n",
      "Beginning import: invbot\n",
      "Completed import: invbot\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = importData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONSTRUCT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(hidden_neurons, activation=activation_fn, input_dim=input_size))\n",
    "model.add(Dense(hidden_neurons, activation=activation_fn))\n",
    "model.add(Dense(output_size, activation=output_fn))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.3014 - acc: 0.9071 - val_loss: 0.2053 - val_acc: 0.9308\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0983 - acc: 0.9696 - val_loss: 0.1261 - val_acc: 0.9622\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0638 - acc: 0.9803 - val_loss: 0.0629 - val_acc: 0.9794\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0434 - acc: 0.9865 - val_loss: 0.0645 - val_acc: 0.9803\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.0289 - acc: 0.9913 - val_loss: 0.0570 - val_acc: 0.9821\n",
      "Test accuracy: 0.9821\n"
     ]
    }
   ],
   "source": [
    "modelx = Sequential()\n",
    "modelx.add(Dense(hidden_neurons, activation=activation_fn, input_dim=input_size))\n",
    "modelx.add(Dense(hidden_neurons, activation=activation_fn))\n",
    "modelx.add(Dense(output_size, activation=output_fn))\n",
    "modelx.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelx.fit(data[1][\"train\"], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(data[1][\"test\"], y_test))\n",
    "score = modelx.evaluate(data[1][\"test\"], y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 1.5657 - acc: 0.5134 - val_loss: 1.0039 - val_acc: 0.6727\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.5083 - acc: 0.8320 - val_loss: 0.2934 - val_acc: 0.9063\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.3421 - acc: 0.8905 - val_loss: 0.5574 - val_acc: 0.8176\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 188s 3ms/step - loss: 0.2703 - acc: 0.9130 - val_loss: 0.2215 - val_acc: 0.9290\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 180s 3ms/step - loss: 0.2314 - acc: 0.9267 - val_loss: 0.2435 - val_acc: 0.9221\n",
      "Test loss: 0.24352554704993964\n",
      "Test accuracy: 0.9221\n"
     ]
    }
   ],
   "source": [
    "model.fit(data[0][\"train\"], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(data[0][\"test\"], y_test))\n",
    "score = model.evaluate(data[0][\"test\"], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save trained model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 183s 3ms/step - loss: 0.3134 - acc: 0.9108 - val_loss: 0.1967 - val_acc: 0.9326\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.1040 - acc: 0.9681 - val_loss: 0.1114 - val_acc: 0.9652\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 262s 4ms/step - loss: 0.0654 - acc: 0.9800 - val_loss: 0.0797 - val_acc: 0.9744\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 287s 5ms/step - loss: 0.0465 - acc: 0.9859 - val_loss: 0.0732 - val_acc: 0.9782\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 284s 5ms/step - loss: 0.0328 - acc: 0.9901 - val_loss: 0.0650 - val_acc: 0.9800\n",
      "Test accuracy 1: 0.9221\n",
      "Test accuracy 2: 0.0982\n"
     ]
    }
   ],
   "source": [
    "# Reload model\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "model2.fit(data[1][\"train\"], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(data[1][\"test\"], y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 1: 0.2351\n",
      "Test accuracy 2: 0.98\n"
     ]
    }
   ],
   "source": [
    "score1 = model2.evaluate(data[0][\"test\"], y_test, verbose=0)\n",
    "score2 = model2.evaluate(data[1][\"test\"], y_test, verbose=0)\n",
    "print('Test accuracy 1:', score1[1])\n",
    "print('Test accuracy 2:', score2[1])\n",
    "\n",
    "# Save trained model\n",
    "model2.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 275s 5ms/step - loss: 0.2718 - acc: 0.9290 - val_loss: 0.1007 - val_acc: 0.9682\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 269s 4ms/step - loss: 0.0797 - acc: 0.9753 - val_loss: 0.1050 - val_acc: 0.9653\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 286s 5ms/step - loss: 0.0501 - acc: 0.9844 - val_loss: 0.0678 - val_acc: 0.9784\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 250s 4ms/step - loss: 0.0334 - acc: 0.9903 - val_loss: 0.0767 - val_acc: 0.9759\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 248s 4ms/step - loss: 0.0220 - acc: 0.9939 - val_loss: 0.0705 - val_acc: 0.9782\n",
      "Test accuracy 1: 0.1178\n",
      "Test accuracy 2: 0.3358\n",
      "Test accuracy 3: 0.9782\n"
     ]
    }
   ],
   "source": [
    "model3 = load_model('model2.h5')\n",
    "\n",
    "model3.fit(data[2][\"train\"], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(data[2][\"test\"], y_test))\n",
    "score1 = model3.evaluate(data[0][\"test\"], y_test, verbose=0)\n",
    "score2 = model3.evaluate(data[1][\"test\"], y_test, verbose=0)\n",
    "score3 = model3.evaluate(data[2][\"test\"], y_test, verbose=0)\n",
    "print('Test accuracy 1:', score1[1])\n",
    "print('Test accuracy 2:', score2[1])\n",
    "print('Test accuracy 3:', score3[1])\n",
    "\n",
    "model3.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.2233 - acc: 0.9437 - val_loss: 0.0884 - val_acc: 0.9731\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0638 - acc: 0.9806 - val_loss: 0.0610 - val_acc: 0.9780\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 139s 2ms/step - loss: 0.0395 - acc: 0.9874 - val_loss: 0.0647 - val_acc: 0.9793\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 110s 2ms/step - loss: 0.0249 - acc: 0.9926 - val_loss: 0.0709 - val_acc: 0.9766\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0163 - acc: 0.9955 - val_loss: 0.0575 - val_acc: 0.9816\n",
      "Test accuracy 1: 0.1631\n",
      "Test accuracy 2: 0.1919\n",
      "Test accuracy 3: 0.4445\n",
      "Test accuracy 4: 0.9816\n"
     ]
    }
   ],
   "source": [
    "# Reload model\n",
    "model4 = load_model('model3.h5')\n",
    "\n",
    "# Continue training\n",
    "model4.fit(data[3][\"train\"], y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          verbose=1,\n",
    "          validation_data=(data[3][\"test\"], y_test))\n",
    "score1 = model4.evaluate(data[0][\"test\"], y_test, verbose=0)\n",
    "score2 = model4.evaluate(data[1][\"test\"], y_test, verbose=0)\n",
    "score3 = model4.evaluate(data[2][\"test\"], y_test, verbose=0)\n",
    "score4 = model4.evaluate(data[3][\"test\"], y_test, verbose=0)\n",
    "\n",
    "print('Test accuracy 1:', score1[1])\n",
    "print('Test accuracy 2:', score2[1])\n",
    "print('Test accuracy 3:', score3[1])\n",
    "print('Test accuracy 4:', score4[1])\n",
    "\n",
    "# Save trained model\n",
    "model4.save('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.7884 - acc: 0.7661 - val_loss: 0.2741 - val_acc: 0.9085\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.2635 - acc: 0.9169 - val_loss: 0.1934 - val_acc: 0.9413\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1764 - acc: 0.9447 - val_loss: 0.1446 - val_acc: 0.9525\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.1328 - acc: 0.9583 - val_loss: 0.1363 - val_acc: 0.9575\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.1231 - acc: 0.9608 - val_loss: 0.2707 - val_acc: 0.9143\n",
      "Test accuracy 1: 0.109\n",
      "Test accuracy 2: 0.1684\n",
      "Test accuracy 3: 0.4884\n",
      "Test accuracy 4: 0.9745\n",
      "Test accuracy 5: 0.9143\n"
     ]
    }
   ],
   "source": [
    "model5 = load_model('model4.h5')\n",
    "\n",
    "# Continue training\n",
    "model5.fit(data[4][\"train\"], y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          verbose=1,\n",
    "          validation_data=(data[4][\"test\"], y_test))\n",
    "score1 = model5.evaluate(data[0][\"test\"], y_test, verbose=0)\n",
    "score2 = model5.evaluate(data[1][\"test\"], y_test, verbose=0)\n",
    "score3 = model5.evaluate(data[2][\"test\"], y_test, verbose=0)\n",
    "score4 = model5.evaluate(data[3][\"test\"], y_test, verbose=0)\n",
    "score5 = model5.evaluate(data[4][\"test\"], y_test, verbose=0)\n",
    "\n",
    "print('Test accuracy 1:', score1[1])\n",
    "print('Test accuracy 2:', score2[1])\n",
    "print('Test accuracy 3:', score3[1])\n",
    "print('Test accuracy 4:', score4[1])\n",
    "print('Test accuracy 5:', score5[1])\n",
    "\n",
    "\n",
    "# Save trained model\n",
    "model5.save('model5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 183s 3ms/step - loss: 0.1640 - acc: 0.9540 - val_loss: 0.0891 - val_acc: 0.9708\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 176s 3ms/step - loss: 0.0518 - acc: 0.9837 - val_loss: 0.0648 - val_acc: 0.9788\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 124s 2ms/step - loss: 0.0313 - acc: 0.9902 - val_loss: 0.0555 - val_acc: 0.9837\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0188 - acc: 0.9943 - val_loss: 0.0537 - val_acc: 0.9837\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0842 - val_acc: 0.9760\n",
      "Test accuracy 1: 0.0946\n",
      "Test accuracy 2: 0.1595\n",
      "Test accuracy 3: 0.5223\n",
      "Test accuracy 4: 0.4856\n",
      "Test accuracy 5: 0.908\n",
      "Test accuracy 6: 0.976\n"
     ]
    }
   ],
   "source": [
    "model6 = load_model('model5.h5')\n",
    "\n",
    "# Continue training\n",
    "model6.fit(data[5][\"train\"], y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          verbose=1,\n",
    "          validation_data=(data[5][\"test\"], y_test))\n",
    "score1 = model6.evaluate(data[0][\"test\"], y_test, verbose=0)\n",
    "score2 = model6.evaluate(data[1][\"test\"], y_test, verbose=0)\n",
    "score3 = model6.evaluate(data[2][\"test\"], y_test, verbose=0)\n",
    "score4 = model6.evaluate(data[3][\"test\"], y_test, verbose=0)\n",
    "score5 = model6.evaluate(data[4][\"test\"], y_test, verbose=0)\n",
    "score6 = model6.evaluate(data[5][\"test\"], y_test, verbose=0)\n",
    "\n",
    "print('Test accuracy 1:', score1[1])\n",
    "print('Test accuracy 2:', score2[1])\n",
    "print('Test accuracy 3:', score3[1])\n",
    "print('Test accuracy 4:', score4[1])\n",
    "print('Test accuracy 5:', score5[1])\n",
    "print('Test accuracy 6:', score6[1])\n",
    "\n",
    "# Save trained model\n",
    "model6.save('model6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.3105 - acc: 0.9293 - val_loss: 0.1095 - val_acc: 0.9644\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0739 - acc: 0.9770 - val_loss: 0.1338 - val_acc: 0.9572\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0422 - acc: 0.9868 - val_loss: 0.0789 - val_acc: 0.9769\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0256 - acc: 0.9927 - val_loss: 0.0571 - val_acc: 0.9836\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.0142 - acc: 0.9961 - val_loss: 0.0618 - val_acc: 0.9826\n",
      "Test accuracy 1: 0.0641\n",
      "Test accuracy 2: 0.1241\n",
      "Test accuracy 3: 0.5182\n",
      "Test accuracy 4: 0.4298\n",
      "Test accuracy 5: 0.8903\n",
      "Test accuracy 6: 0.9086\n",
      "Test accuracy 7: 0.9826\n"
     ]
    }
   ],
   "source": [
    "model7 = load_model('model6.h5')\n",
    "\n",
    "# Continue training\n",
    "model7.fit(data[6][\"train\"], y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          verbose=1,\n",
    "          validation_data=(data[6][\"test\"], y_test))\n",
    "score1 = model7.evaluate(data[0][\"test\"], y_test, verbose=0)\n",
    "score2 = model7.evaluate(data[1][\"test\"], y_test, verbose=0)\n",
    "score3 = model7.evaluate(data[2][\"test\"], y_test, verbose=0)\n",
    "score4 = model7.evaluate(data[3][\"test\"], y_test, verbose=0)\n",
    "score5 = model7.evaluate(data[4][\"test\"], y_test, verbose=0)\n",
    "score6 = model7.evaluate(data[5][\"test\"], y_test, verbose=0)\n",
    "score7 = model7.evaluate(data[6][\"test\"], y_test, verbose=0)\n",
    "\n",
    "print('Test accuracy 1:', score1[1])\n",
    "print('Test accuracy 2:', score2[1])\n",
    "print('Test accuracy 3:', score3[1])\n",
    "print('Test accuracy 4:', score4[1])\n",
    "print('Test accuracy 5:', score5[1])\n",
    "print('Test accuracy 6:', score6[1])\n",
    "print('Test accuracy 7:', score7[1])\n",
    "\n",
    "# Save trained model\n",
    "model7.save('model7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 108s 2ms/step - loss: 0.4957 - acc: 0.8769 - val_loss: 0.2993 - val_acc: 0.9036\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.1750 - acc: 0.9463 - val_loss: 0.4134 - val_acc: 0.8785\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.1254 - acc: 0.9615 - val_loss: 0.2048 - val_acc: 0.9253\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.1225 - acc: 0.9655 - val_loss: 0.0849 - val_acc: 0.9729\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0813 - acc: 0.9746 - val_loss: 0.0824 - val_acc: 0.9732\n",
      "Test accuracy 1: 0.1685\n",
      "Test accuracy 2: 0.1415\n",
      "Test accuracy 3: 0.4592\n",
      "Test accuracy 4: 0.37\n",
      "Test accuracy 5: 0.7466\n",
      "Test accuracy 6: 0.7098\n",
      "Test accuracy 7: 0.8192\n",
      "Test accuracy 7: 0.9732\n"
     ]
    }
   ],
   "source": [
    "model8 = load_model('model7.h5')\n",
    "\n",
    "# Continue training\n",
    "model8.fit(data[7][\"train\"], y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          verbose=1,\n",
    "          validation_data=(data[7][\"test\"], y_test))\n",
    "score1 = model8.evaluate(data[0][\"test\"], y_test, verbose=0)\n",
    "score2 = model8.evaluate(data[1][\"test\"], y_test, verbose=0)\n",
    "score3 = model8.evaluate(data[2][\"test\"], y_test, verbose=0)\n",
    "score4 = model8.evaluate(data[3][\"test\"], y_test, verbose=0)\n",
    "score5 = model8.evaluate(data[4][\"test\"], y_test, verbose=0)\n",
    "score6 = model8.evaluate(data[5][\"test\"], y_test, verbose=0)\n",
    "score7 = model8.evaluate(data[6][\"test\"], y_test, verbose=0)\n",
    "score8 = model8.evaluate(data[7][\"test\"], y_test, verbose=0)\n",
    "\n",
    "\n",
    "print('Test accuracy 1:', score1[1])\n",
    "print('Test accuracy 2:', score2[1])\n",
    "print('Test accuracy 3:', score3[1])\n",
    "print('Test accuracy 4:', score4[1])\n",
    "print('Test accuracy 5:', score5[1])\n",
    "print('Test accuracy 6:', score6[1])\n",
    "print('Test accuracy 7:', score7[1])\n",
    "print('Test accuracy 7:', score8[1])\n",
    "\n",
    "# Save trained model\n",
    "model8.save('model8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs = input_layer,\n",
    "        filters = 32,\n",
    "        kernel_size=[5,5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "\n",
    "    # Pooling 1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional layer 2 and pooling layer\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5,5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "\n",
    "    # Pooling 2 with flattening\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2)\n",
    "    pool2_flat=tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense layer with dropout \n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout=tf.layers.dropout(inputs=dense, rate=0.4, training=mode==tf.estimator.ModeKeys.TRAIN)\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Caluclate loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure training op\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n",
    "# Logging predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, \n",
    "    every_n_iter=50)\n",
    "\n",
    "# Our application logic will be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    # Training on original\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\":train_original},\n",
    "        y=train_labels,\n",
    "        batch_size=1000,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    \n",
    "    mnist_classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=200,\n",
    "        hooks=[logging_hook])\n",
    "\n",
    "    # Evaluation on original\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": eval_original},\n",
    "        y=eval_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    \n",
    "    eval_results=mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
