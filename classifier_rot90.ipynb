{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy \n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import gzip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs = input_layer,\n",
    "        filters = 32,\n",
    "        kernel_size=[5,5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "\n",
    "    # Pooling 1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional layer 2 and pooling layer\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5,5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "\n",
    "    # Pooling 2 with flattening\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2)\n",
    "    pool2_flat=tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense layer with dropout \n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout=tf.layers.dropout(inputs=dense, rate=0.4, training=mode==tf.estimator.ModeKeys.TRAIN)\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Caluclate loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure training op\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/mnist_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x181ce93cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n",
    "# Logging predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, \n",
    "    every_n_iter=50)\n",
    "\n",
    "# Our application logic will be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Extract labels from MNIST labels into vector\n",
    "def extract_labels(filename, num_images):\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "  return labels\n",
    "\n",
    "train_labels = extract_labels(\"MNIST-data/train-labels-idx1-ubyte.gz\", 60000)\n",
    "eval_labels = extract_labels(\"MNIST-data/t10k-labels-idx1-ubyte.gz\", 10000)\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(eval_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "\n",
    "# train_data_int = mnist.train.images\n",
    "# train_labels_int = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "# eval_data_int = mnist.test.images\n",
    "# eval_labels_int = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "# print(np.shape(train_data_int))\n",
    "# print(np.shape(train_labels_int))\n",
    "# print(np.shape(eval_data_int))\n",
    "# print(np.shape(eval_labels_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_img(filename, label):\n",
    "#     img_str = tf.read_file(filename)\n",
    "#     img_dec = tf.image.decode_image(img_str)\n",
    "# #     img_res = tf.image.resize_images(img_dec, [28, 28])\n",
    "#     return img_dec, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = \"MNIST-processed-training/rot90/rot901.png\"\n",
    "# print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# # Load rotated images for training\n",
    "# filenames = tf.constant([\"MNIST-processed-training/rot90/rot90{0}.png\".format(k) for k in range(1,60001)])\n",
    "\n",
    "# labels = train_labels\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "# dataset = dataset.map(parse_img)\n",
    "    \n",
    "# print(np.shape(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape(None), TensorShape([]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load rotated images for training\n",
    "# images = [\"MNIST-processed-training/rot90/rot90{0}.png\".format(k) for k in range(1,60001)]\n",
    "\n",
    "# train_data = [] \n",
    "# for image in images: \n",
    "#     img = Image.open(image)\n",
    "#     arr = np.array(img)\n",
    "#     train_data.append(arr)\n",
    "# #     train_data.append(imageio.imread(image))\n",
    "    \n",
    "# # Flatten images    \n",
    "# for i in range(len(train_data)):\n",
    "#     train_data[i] = np.array(train_data[i].flatten())\n",
    "    \n",
    "# print(np.shape(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# ROTATE 90 train\n",
    "train_rot90 = np.zeros((60000,28,28), dtype=np.float32)\n",
    "images_rot90 = [\"MNIST-processed-training/rot90/rot90{0}.png\".format(k) for k in range(1,60000)]\n",
    "\n",
    "for i in range(len(images_rot90)):\n",
    "    img = np.array(Image.open(images_rot90[i]))\n",
    "    train_rot90[i, :, :] = img\n",
    " \n",
    "print(np.shape(train_rot90))\n",
    "\n",
    "# ROTATE 90 test\n",
    "eval_rot90 = np.zeros((10000,28,28), dtype=np.float32)\n",
    "images2_rot90 = [\"MNIST-processed-test/rot90/test-rot90{0}.png\".format(k) for k in range(1,10001)]\n",
    "\n",
    "for i in range(len(images2_rot90)):\n",
    "    img = np.array(Image.open(images2_rot90[i]))\n",
    "    eval_rot90[i, :, :] = img\n",
    " \n",
    "print(np.shape(eval_rot90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# INV train\n",
    "train_inv = np.zeros((60000,28,28), dtype=np.float32)\n",
    "images_inv = [\"MNIST-processed-training/Inv/inv{0}.png\".format(k) for k in range(1,60000)]\n",
    "\n",
    "for i in range(len(images_inv)):\n",
    "    img = np.array(Image.open(images_inv[i]))\n",
    "    train_inv[i, :, :] = img\n",
    " \n",
    "print(np.shape(train_inv))\n",
    "\n",
    "# INV test\n",
    "eval_inv = np.zeros((10000,28,28), dtype=np.float32)\n",
    "images2_inv = [\"MNIST-processed-test/inv/test-inv{0}.png\".format(k) for k in range(1,10001)]\n",
    "\n",
    "for i in range(len(images2_inv)):\n",
    "    img = np.array(Image.open(images2_inv[i]))\n",
    "    eval_inv[i, :, :] = img\n",
    " \n",
    "print(np.shape(eval_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fliplr train\n",
    "train_fliplr = np.zeros((60000,28,28), dtype=np.float32)\n",
    "images_fliplr = [\"MNIST-processed-training/fliplr/fliplr{0}.png\".format(k) for k in range(1,60000)]\n",
    "\n",
    "for i in range(len(images_fliplr)):\n",
    "    img = np.array(Image.open(images_fliplr[i]))\n",
    "    train_fliplr[i, :, :] = img\n",
    " \n",
    "print(np.shape(train_fliplr))\n",
    "\n",
    "# fliplr test\n",
    "eval_fliplr = np.zeros((10000,28,28), dtype=np.float32)\n",
    "images2_fliplr = [\"MNIST-processed-test/fliplr/test-fliplr{0}.png\".format(k) for k in range(1,10001)]\n",
    "\n",
    "for i in range(len(images2_fliplr)):\n",
    "    img = np.array(Image.open(images2_fliplr[i]))\n",
    "    eval_fliplr[i, :, :] = img\n",
    " \n",
    "print(np.shape(eval_fliplr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutud train\n",
    "train_cutud = np.zeros((60000,28,28), dtype=np.float32)\n",
    "images_cutud = [\"MNIST-processed-training/cutud/cutUD{0}.png\".format(k) for k in range(1,60000)]\n",
    "\n",
    "for i in range(len(images_cutud)):\n",
    "    img = np.array(Image.open(images_cutud[i]))\n",
    "    train_cutud[i, :, :] = img\n",
    " \n",
    "print(np.shape(train_cutud))\n",
    "\n",
    "# cutud test\n",
    "eval_cutud = np.zeros((10000,28,28), dtype=np.float32)\n",
    "images2_cutud = [\"MNIST-processed-test/cutud/test-cutud{0}.png\".format(k) for k in range(1,10001)]\n",
    "\n",
    "for i in range(len(images2_cutud)):\n",
    "    img = np.array(Image.open(images2_cutud[i]))\n",
    "    eval_cutud[i, :, :] = img\n",
    " \n",
    "print(np.shape(eval_cutud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkerboard train\n",
    "train_checkerboard = np.zeros((60000,28,28), dtype=np.float32)\n",
    "images_checkerboard = [\"MNIST-processed-training/checkerboard/fullcheck{0}.png\".format(k) for k in range(1,60000)]\n",
    "\n",
    "for i in range(len(images_checkerboard)):\n",
    "    img = np.array(Image.open(images_checkerboard[i]))\n",
    "    train_checkerboard[i, :, :] = img\n",
    " \n",
    "print(np.shape(train_checkerboard))\n",
    "\n",
    "# checkerboard test\n",
    "eval_checkerboard = np.zeros((10000,28,28), dtype=np.float32)\n",
    "images2_checkerboard = [\"MNIST-processed-test/checkerboard/test-checkerboard{0}.png\".format(k) for k in range(1,10001)]\n",
    "\n",
    "for i in range(len(images2_checkerboard)):\n",
    "    img = np.array(Image.open(images2_checkerboard[i]))\n",
    "    eval_checkerboard[i, :, :] = img\n",
    " \n",
    "print(np.shape(eval_checkerboard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flipud train\n",
    "train_flipud = np.zeros((60000,28,28), dtype=np.float32)\n",
    "images_flipud = [\"MNIST-processed-training/flipud/flipud{0}.png\".format(k) for k in range(1,60000)]\n",
    "\n",
    "for i in range(len(images_flipud)):\n",
    "    img = np.array(Image.open(images_flipud[i]))\n",
    "    train_flipud[i, :, :] = img\n",
    " \n",
    "print(np.shape(train_data))\n",
    "\n",
    "# flipud test\n",
    "eval_flipud = np.zeros((10000,28,28), dtype=np.float32)\n",
    "images2_flipud = [\"MNIST-processed-test/flipud/test-flipud{0}.png\".format(k) for k in range(1,10001)]\n",
    "\n",
    "for i in range(len(images2_flipud)):\n",
    "    img = np.array(Image.open(images2_flipud[i]))\n",
    "    eval_flipud[i, :, :] = img\n",
    " \n",
    "print(np.shape(eval_flipud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invbot train\n",
    "train_invbot = np.zeros((60000,28,28), dtype=np.float32)\n",
    "images_invbot = [\"MNIST-processed-training/invbot/invbot{0}.png\".format(k) for k in range(1,60000)]\n",
    "\n",
    "for i in range(len(images_invbot)):\n",
    "    img = np.array(Image.open(images_invbot[i]))\n",
    "    train_invbot[i, :, :] = img\n",
    " \n",
    "print(np.shape(train_invbot))\n",
    "\n",
    "# invbot test\n",
    "eval_invbot = np.zeros((10000,28,28), dtype=np.float32)\n",
    "images2_invbot = [\"MNIST-processed-test/invbot/test-invbot{0}.png\".format(k) for k in range(1,10001)]\n",
    "\n",
    "for i in range(len(images2_invbot)):\n",
    "    img = np.array(Image.open(images2_invbot[i]))\n",
    "    eval_invbot[i, :, :] = img\n",
    " \n",
    "print(np.shape(eval_invbot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tensorflow MNIST data\n",
    "# def main(unused_argv):\n",
    "#     # Training\n",
    "#     train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#         x={\"x\":train_data_int},\n",
    "#         y=train_labels_int,\n",
    "#         batch_size=100,\n",
    "#         num_epochs=None,\n",
    "#         shuffle=True)\n",
    "    \n",
    "#     mnist_classifier.train(\n",
    "#         input_fn=train_input_fn,\n",
    "#         steps=200,\n",
    "#         hooks=[logging_hook])\n",
    "\n",
    "#     # Evaluation\n",
    "#     eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#         x={\"x\": eval_data_int},\n",
    "#         y=eval_labels_int,\n",
    "#         num_epochs=1,\n",
    "#         shuffle=False)\n",
    "    \n",
    "#     eval_results=mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "#     print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    # Training on rot90\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\":train_rot90},\n",
    "        y=train_labels,\n",
    "        batch_size=1000,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    \n",
    "    mnist_classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=200,\n",
    "        hooks=[logging_hook])\n",
    "\n",
    "    # Evaluation on rot90\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": eval_rot90},\n",
    "        y=eval_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    \n",
    "    eval_results=mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-4201\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4202 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[0.00000215 0.99941874 0.00001955 ... 0.00028886 0.00002232 0.00000258]\n",
      " [0.         0.00000005 0.0000252  ... 0.00000028 0.0000002  0.00000024]\n",
      " [0.8263984  0.00000012 0.16598742 ... 0.00005144 0.00257748 0.00018237]\n",
      " ...\n",
      " [0.00000948 0.00000874 0.00018036 ... 0.00018195 0.99227834 0.00678182]\n",
      " [0.00000207 0.00026185 0.06254628 ... 0.00136261 0.00472883 0.05135954]\n",
      " [0.00000341 0.00000006 0.00000002 ... 0.00000001 0.00000158 0.0000002 ]]\n",
      "INFO:tensorflow:loss = 0.20579624, step = 4202\n",
      "INFO:tensorflow:probabilities = [[0.00000258 0.         0.00046831 ... 0.99950767 0.00000002 0.00001606]\n",
      " [0.00003144 0.00146197 0.00261745 ... 0.00290935 0.02462151 0.00078083]\n",
      " [0.00000103 0.         0.00000344 ... 0.00007898 0.00000224 0.9998481 ]\n",
      " ...\n",
      " [0.00000046 0.00007348 0.00000007 ... 0.0002018  0.00000138 0.00074579]\n",
      " [0.00005405 0.00004794 0.00007278 ... 0.00223626 0.0068832  0.00281847]\n",
      " [0.00001218 0.00001652 0.17564593 ... 0.00024847 0.00114284 0.00001626]] (139.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.378179\n",
      "INFO:tensorflow:probabilities = [[0.00026095 0.00000349 0.00000049 ... 0.00000008 0.00000276 0.00000012]\n",
      " [0.00000858 0.00003007 0.00022344 ... 0.9995443  0.0000035  0.00007283]\n",
      " [0.00001405 0.00000297 0.00021554 ... 0.00000038 0.9902672  0.00001061]\n",
      " ...\n",
      " [0.00000004 0.9999976  0.00000157 ... 0.00000011 0.00000025 0.00000004]\n",
      " [0.00000389 0.00000019 0.00000144 ... 0.00014477 0.00000998 0.99854183]\n",
      " [0.00000162 0.99947435 0.00016463 ... 0.00006728 0.00004072 0.00000045]] (125.113 sec)\n",
      "INFO:tensorflow:loss = 0.12909174, step = 4302 (264.434 sec)\n",
      "INFO:tensorflow:probabilities = [[0.00444947 0.00061638 0.00015107 ... 0.00027416 0.0013027  0.96028763]\n",
      " [0.00000218 0.00000012 0.0000001  ... 0.00001652 0.00001899 0.9984952 ]\n",
      " [0.00000397 0.99971026 0.00004837 ... 0.00010829 0.00001274 0.0000066 ]\n",
      " ...\n",
      " [0.00014053 0.00000038 0.00000008 ... 0.00000008 0.0000012  0.00008602]\n",
      " [0.9972379  0.00000019 0.00169274 ... 0.         0.00007809 0.00000014]\n",
      " [0.0001386  0.9988356  0.00011787 ... 0.00002806 0.00058749 0.00002156]] (124.049 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4401 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.115461536.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-17-21:27:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-4401\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-17-21:27:46\n",
      "INFO:tensorflow:Saving dict for global step 4401: accuracy = 0.9736, global_step = 4401, loss = 0.08281821\n",
      "{'accuracy': 0.9736, 'loss': 0.08281821, 'global_step': 4401}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Key logits not found in checkpoint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-19ba7d2df584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_value\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'logits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mget_variable_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \"\"\"\n\u001b[1;32m    263\u001b[0m     \u001b[0m_check_checkpoint_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_variable_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_variable\u001b[0;34m(ckpt_dir_or_file, name)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),\n\u001b[0;32m--> 318\u001b[0;31m                                           status)\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_CheckpointReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key logits not found in checkpoint"
     ]
    }
   ],
   "source": [
    "mnist_classifier.get_variable_value( 'logits')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
